{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48aecd4-cb8c-4f10-a855-5a0d39f0e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 6/10/21 11:00 PM\n",
    "# @Author  : Yuan Gong\n",
    "# @Affiliation  : Massachusetts Institute of Technology\n",
    "# @Email   : yuangong@mit.edu\n",
    "# @File    : traintest.py\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "sys.path.append(os.path.dirname(os.path.dirname(sys.path[0])))\n",
    "from utilities import *\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "\n",
    "def train(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('running on ' + str(device))\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    # Initialize all of the statistics we want to keep track of\n",
    "    batch_time = AverageMeter()\n",
    "    per_sample_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    per_sample_data_time = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    per_sample_dnn_time = AverageMeter()\n",
    "    progress = []\n",
    "    # best_cum_mAP is checkpoint ensemble from the first epoch to the best epoch\n",
    "    best_epoch, best_cum_epoch, best_mAP, best_acc, best_cum_mAP = 0, 0, -np.inf, -np.inf, -np.inf\n",
    "    global_step, epoch = 0, 0\n",
    "    start_time = time.time()\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    def _save_progress():\n",
    "        progress.append([epoch, global_step, best_epoch, best_mAP,\n",
    "                time.time() - start_time])\n",
    "        with open(\"%s/progress.pkl\" % exp_dir, \"wb\") as f:\n",
    "            pickle.dump(progress, f)\n",
    "\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.3f} million'.format(sum(p.numel() for p in audio_model.parameters()) / 1e6))\n",
    "    print('Total trainable parameter number is : {:.3f} million'.format(sum(p.numel() for p in trainables) / 1e6))\n",
    "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "\n",
    "    # dataset specific settings\n",
    "    #scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=args.lr_patience, verbose=True)\n",
    "    if args.dataset == 'audioset':\n",
    "        if len(train_loader.dataset) > 2e5:\n",
    "            print('scheduler for full audioset is used')\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2,3,4,5], gamma=0.5, last_epoch=-1)\n",
    "        else:\n",
    "            print('scheduler for balanced audioset is used')\n",
    "            scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10, 15, 20, 25], gamma=0.5, last_epoch=-1)\n",
    "        main_metrics = 'mAP'\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        warmup = True\n",
    "    elif args.dataset == 'esc50':\n",
    "        print('scheduler for esc-50 is used')\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
    "        main_metrics = 'acc'\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        warmup = False\n",
    "    elif args.dataset == 'speechcommands':\n",
    "        print('scheduler for speech commands is used')\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(5,26)), gamma=0.85)\n",
    "        main_metrics = 'acc'\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        warmup = False\n",
    "    else:\n",
    "        raise ValueError('unknown dataset, dataset should be in [audioset, speechcommands, esc50]')\n",
    "    print('now training with {:s}, main metrics: {:s}, loss function: {:s}, learning rate scheduler: {:s}'.format(str(args.dataset), str(main_metrics), str(loss_fn), str(scheduler)))\n",
    "    args.loss_fn = loss_fn\n",
    "\n",
    "    epoch += 1\n",
    "    # for amp\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "    result = np.zeros([args.n_epochs, 10])\n",
    "    audio_model.train()\n",
    "    while epoch < args.n_epochs + 1:\n",
    "        begin_time = time.time()\n",
    "        end_time = time.time()\n",
    "        audio_model.train()\n",
    "        print('---------------')\n",
    "        print(datetime.datetime.now())\n",
    "        print(\"current #epochs=%s, #steps=%s\" % (epoch, global_step))\n",
    "\n",
    "        for i, (audio_input, labels) in enumerate(train_loader):\n",
    "\n",
    "            B = audio_input.size(0)\n",
    "            audio_input = audio_input.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            data_time.update(time.time() - end_time)\n",
    "            per_sample_data_time.update((time.time() - end_time) / audio_input.shape[0])\n",
    "            dnn_start_time = time.time()\n",
    "\n",
    "            # first several steps for warm-up\n",
    "            if global_step <= 1000 and global_step % 50 == 0 and warmup == True:\n",
    "                warm_lr = (global_step / 1000) * args.lr\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = warm_lr\n",
    "                print('warm-up learning rate is {:f}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            with autocast():\n",
    "                audio_output = audio_model(audio_input)\n",
    "                if isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                    loss = loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "                else:\n",
    "                    loss = loss_fn(audio_output, labels)\n",
    "\n",
    "            # optimization if amp is not used\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # optimiztion if amp is used\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # record loss\n",
    "            loss_meter.update(loss.item(), B)\n",
    "            batch_time.update(time.time() - end_time)\n",
    "            per_sample_time.update((time.time() - end_time)/audio_input.shape[0])\n",
    "            per_sample_dnn_time.update((time.time() - dnn_start_time)/audio_input.shape[0])\n",
    "\n",
    "            print_step = global_step % args.n_print_steps == 0\n",
    "            early_print_step = epoch == 0 and global_step % (args.n_print_steps/10) == 0\n",
    "            print_step = print_step or early_print_step\n",
    "\n",
    "            if print_step and global_step != 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Per Sample Total Time {per_sample_time.avg:.5f}\\t'\n",
    "                  'Per Sample Data Time {per_sample_data_time.avg:.5f}\\t'\n",
    "                  'Per Sample DNN Time {per_sample_dnn_time.avg:.5f}\\t'\n",
    "                  'Train Loss {loss_meter.avg:.4f}\\t'.format(\n",
    "                   epoch, i, len(train_loader), per_sample_time=per_sample_time, per_sample_data_time=per_sample_data_time,\n",
    "                      per_sample_dnn_time=per_sample_dnn_time, loss_meter=loss_meter), flush=True)\n",
    "                if np.isnan(loss_meter.avg):\n",
    "                    print(\"training diverged...\")\n",
    "                    return\n",
    "\n",
    "            end_time = time.time()\n",
    "            global_step += 1\n",
    "\n",
    "        print('start validation')\n",
    "        stats, valid_loss = validate(audio_model, test_loader, args, epoch)\n",
    "\n",
    "        # ensemble results\n",
    "        cum_stats = validate_ensemble(args, epoch)\n",
    "        cum_mAP = np.mean([stat['AP'] for stat in cum_stats])\n",
    "        cum_mAUC = np.mean([stat['auc'] for stat in cum_stats])\n",
    "        cum_acc = cum_stats[0]['acc']\n",
    "\n",
    "        mAP = np.mean([stat['AP'] for stat in stats])\n",
    "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "        acc = stats[0]['acc']\n",
    "\n",
    "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "        average_precision = np.mean(middle_ps)\n",
    "        average_recall = np.mean(middle_rs)\n",
    "\n",
    "        if main_metrics == 'mAP':\n",
    "            print(\"mAP: {:.6f}\".format(mAP))\n",
    "        else:\n",
    "            print(\"acc: {:.6f}\".format(acc))\n",
    "        print(\"AUC: {:.6f}\".format(mAUC))\n",
    "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "        print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
    "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "\n",
    "        if main_metrics == 'mAP':\n",
    "            result[epoch-1, :] = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_mAP, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
    "        else:\n",
    "            result[epoch-1, :] = [acc, mAUC, average_precision, average_recall, d_prime(mAUC), loss_meter.avg, valid_loss, cum_acc, cum_mAUC, optimizer.param_groups[0]['lr']]\n",
    "        np.savetxt(exp_dir + '/result.csv', result, delimiter=',')\n",
    "        print('validation finished')\n",
    "\n",
    "        if mAP > best_mAP:\n",
    "            best_mAP = mAP\n",
    "            if main_metrics == 'mAP':\n",
    "                best_epoch = epoch\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            if main_metrics == 'acc':\n",
    "                best_epoch = epoch\n",
    "\n",
    "        if cum_mAP > best_cum_mAP:\n",
    "            best_cum_epoch = epoch\n",
    "            best_cum_mAP = cum_mAP\n",
    "\n",
    "        if best_epoch == epoch:\n",
    "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "            torch.save(optimizer.state_dict(), \"%s/models/best_optim_state.pth\" % (exp_dir))\n",
    "\n",
    "        torch.save(audio_model.state_dict(), \"%s/models/audio_model.%d.pth\" % (exp_dir, epoch))\n",
    "        if len(train_loader.dataset) > 2e5:\n",
    "            torch.save(optimizer.state_dict(), \"%s/models/optim_state.%d.pth\" % (exp_dir, epoch))\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        with open(exp_dir + '/stats_' + str(epoch) +'.pickle', 'wb') as handle:\n",
    "            pickle.dump(stats, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        _save_progress()\n",
    "\n",
    "        finish_time = time.time()\n",
    "        print('epoch {:d} training time: {:.3f}'.format(epoch, finish_time-begin_time))\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "        batch_time.reset()\n",
    "        per_sample_time.reset()\n",
    "        data_time.reset()\n",
    "        per_sample_data_time.reset()\n",
    "        loss_meter.reset()\n",
    "        per_sample_dnn_time.reset()\n",
    "\n",
    "    if args.dataset == 'audioset':\n",
    "        if len(train_loader.dataset) > 2e5:\n",
    "            stats=validate_wa(audio_model, test_loader, args, 1, 5)\n",
    "        else:\n",
    "            stats=validate_wa(audio_model, test_loader, args, 6, 25)\n",
    "        mAP = np.mean([stat['AP'] for stat in stats])\n",
    "        mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "        middle_ps = [stat['precisions'][int(len(stat['precisions'])/2)] for stat in stats]\n",
    "        middle_rs = [stat['recalls'][int(len(stat['recalls'])/2)] for stat in stats]\n",
    "        average_precision = np.mean(middle_ps)\n",
    "        average_recall = np.mean(middle_rs)\n",
    "        wa_result = [mAP, mAUC, average_precision, average_recall, d_prime(mAUC)]\n",
    "        print('---------------Training Finished---------------')\n",
    "        print('weighted averaged model results')\n",
    "        print(\"mAP: {:.6f}\".format(mAP))\n",
    "        print(\"AUC: {:.6f}\".format(mAUC))\n",
    "        print(\"Avg Precision: {:.6f}\".format(average_precision))\n",
    "        print(\"Avg Recall: {:.6f}\".format(average_recall))\n",
    "        print(\"d_prime: {:.6f}\".format(d_prime(mAUC)))\n",
    "        print(\"train_loss: {:.6f}\".format(loss_meter.avg))\n",
    "        print(\"valid_loss: {:.6f}\".format(valid_loss))\n",
    "        np.savetxt(exp_dir + '/wa_result.csv', wa_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b54ae-b418-4b0d-a5fa-c40233fcf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(audio_model, val_loader, args, epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_time = AverageMeter()\n",
    "    if not isinstance(audio_model, nn.DataParallel):\n",
    "        audio_model = nn.DataParallel(audio_model)\n",
    "    audio_model = audio_model.to(device)\n",
    "    # switch to evaluate mode\n",
    "    audio_model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    A_predictions = []\n",
    "    A_targets = []\n",
    "    A_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (audio_input, labels) in enumerate(val_loader):\n",
    "            audio_input = audio_input.to(device)\n",
    "\n",
    "            # compute output\n",
    "            audio_output = audio_model(audio_input)\n",
    "            audio_output = torch.sigmoid(audio_output)\n",
    "            predictions = audio_output.to('cpu').detach()\n",
    "\n",
    "            A_predictions.append(predictions)\n",
    "            A_targets.append(labels)\n",
    "\n",
    "            # compute the loss\n",
    "            labels = labels.to(device)\n",
    "            if isinstance(args.loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = args.loss_fn(audio_output, torch.argmax(labels.long(), axis=1))\n",
    "            else:\n",
    "                loss = args.loss_fn(audio_output, labels)\n",
    "            A_loss.append(loss.to('cpu').detach())\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        audio_output = torch.cat(A_predictions)\n",
    "        target = torch.cat(A_targets)\n",
    "        loss = np.mean(A_loss)\n",
    "        stats = calculate_stats(audio_output, target)\n",
    "\n",
    "        # save the prediction here\n",
    "        exp_dir = args.exp_dir\n",
    "        if os.path.exists(exp_dir+'/predictions') == False:\n",
    "            os.mkdir(exp_dir+'/predictions')\n",
    "            np.savetxt(exp_dir+'/predictions/target.csv', target, delimiter=',')\n",
    "        np.savetxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', audio_output, delimiter=',')\n",
    "\n",
    "    return stats, loss\n",
    "\n",
    "def validate_ensemble(args, epoch):\n",
    "    exp_dir = args.exp_dir\n",
    "    target = np.loadtxt(exp_dir+'/predictions/target.csv', delimiter=',')\n",
    "    if epoch == 1:\n",
    "        cum_predictions = np.loadtxt(exp_dir + '/predictions/predictions_1.csv', delimiter=',')\n",
    "    else:\n",
    "        cum_predictions = np.loadtxt(exp_dir + '/predictions/cum_predictions.csv', delimiter=',') * (epoch - 1)\n",
    "        predictions = np.loadtxt(exp_dir+'/predictions/predictions_' + str(epoch) + '.csv', delimiter=',')\n",
    "        cum_predictions = cum_predictions + predictions\n",
    "        # remove the prediction file to save storage space\n",
    "        os.remove(exp_dir+'/predictions/predictions_' + str(epoch-1) + '.csv')\n",
    "\n",
    "    cum_predictions = cum_predictions / epoch\n",
    "    np.savetxt(exp_dir+'/predictions/cum_predictions.csv', cum_predictions, delimiter=',')\n",
    "\n",
    "    stats = calculate_stats(cum_predictions, target)\n",
    "    return stats\n",
    "\n",
    "def validate_wa(audio_model, val_loader, args, start_epoch, end_epoch):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    sdA = torch.load(exp_dir + '/models/audio_model.' + str(start_epoch) + '.pth', map_location=device)\n",
    "\n",
    "    model_cnt = 1\n",
    "    for epoch in range(start_epoch+1, end_epoch+1):\n",
    "        sdB = torch.load(exp_dir + '/models/audio_model.' + str(epoch) + '.pth', map_location=device)\n",
    "        for key in sdA:\n",
    "            sdA[key] = sdA[key] + sdB[key]\n",
    "        model_cnt += 1\n",
    "\n",
    "        # if choose not to save models of epoch, remove to save space\n",
    "        if args.save_model == False:\n",
    "            os.remove(exp_dir + '/models/audio_model.' + str(epoch) + '.pth')\n",
    "\n",
    "    # averaging\n",
    "    for key in sdA:\n",
    "        sdA[key] = sdA[key] / float(model_cnt)\n",
    "\n",
    "    audio_model.load_state_dict(sdA)\n",
    "\n",
    "    torch.save(audio_model.state_dict(), exp_dir + '/models/audio_model_wa.pth')\n",
    "\n",
    "    stats, loss = validate(audio_model, val_loader, args, 'wa')\n",
    "    return stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
