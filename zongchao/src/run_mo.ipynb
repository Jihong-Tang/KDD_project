{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abdce433-b511-4631-ac89-2230e5bd3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 6/11/21 12:57 AM\n",
    "# @Author  : Yuan Gong\n",
    "# @Affiliation  : Massachusetts Institute of Technology\n",
    "# @Email   : yuangong@mit.edu\n",
    "# @File    : run.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2f1cca4-400c-4ae6-b2c0-e0f035f4fed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zmoad/Documents/Course/06.AST_Audio/AST/src'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fba1f94-d6c8-42b8-85bb-d582de59ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "basepath = os.path.dirname(os.path.dirname(sys.path[0]))\n",
    "sys.path.append(basepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2d8620e-140f-43bb-ad7f-393ed2fa6eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/zmoad/Documents/Course/06.AST_Audio/AST/src',\n",
       " '/Users/zmoad/opt/miniconda3/lib/python39.zip',\n",
       " '/Users/zmoad/opt/miniconda3/lib/python3.9',\n",
       " '/Users/zmoad/opt/miniconda3/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/zmoad/Documents/Course/06.AST_Audio/AST/venvast/lib/python3.9/site-packages',\n",
       " '/Users/zmoad/Documents/Course/06.AST_Audio/AST/venvast/lib/python3.9/site-packages/IPython/extensions',\n",
       " '/Users/zmoad/.ipython',\n",
       " '/Users/zmoad/Documents/Course/06.AST_Audio',\n",
       " '/Users/zmoad/Documents/Course/06.AST_Audio',\n",
       " '/Users/zmoad/Documents/Course/06.AST_Audio']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9845059a-9654-4dee-9813-ef20c6478e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "import models\n",
    "import numpy as np\n",
    "from traintest import train, validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2438d76f-48d8-4423-837e-67f32e023dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am process 19405, running on ZongchaodeiMac.local: starting (Thu Nov 11 21:41:09 2021)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--n_class'], dest='n_class', nargs=None, const=None, default=527, type=<class 'int'>, choices=None, help='number of classes', metavar=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"I am process %s, running on %s: starting (%s)\" % (os.getpid(), os.uname()[1], time.asctime()))\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--data-train\", type=str, default='', help=\"training data json\")\n",
    "parser.add_argument(\"--data-val\", type=str, default='', help=\"validation data json\")\n",
    "parser.add_argument(\"--data-eval\", type=str, default='', help=\"evaluation data json\")\n",
    "parser.add_argument(\"--label-csv\", type=str, default='', help=\"csv with class labels\")\n",
    "parser.add_argument(\"--n_class\", type=int, default=527, help=\"number of classes\")\n",
    "# parser.add_argument(\"--model\", type=str, default='ast', help=\"the model used\")\n",
    "# parser.add_argument(\"--dataset\", type=str, default=\"audioset\", help=\"the dataset used\", choices=[\"audioset\", \"esc50\", \"speechcommands\"])\n",
    "\n",
    "# parser.add_argument(\"--exp-dir\", type=str, default=\"\", help=\"directory to dump experiments\")\n",
    "# parser.add_argument('--lr', '--learning-rate', default=0.001, type=float, metavar='LR', help='initial learning rate')\n",
    "# parser.add_argument(\"--optim\", type=str, default=\"adam\", help=\"training optimizer\", choices=[\"sgd\", \"adam\"])\n",
    "# parser.add_argument('-b', '--batch-size', default=12, type=int, metavar='N', help='mini-batch size')\n",
    "# parser.add_argument('-w', '--num-workers', default=32, type=int, metavar='NW', help='# of workers for dataloading (default: 32)')\n",
    "# parser.add_argument(\"--n-epochs\", type=int, default=1, help=\"number of maximum training epochs\")\n",
    "# # not used in the formal experiments\n",
    "# parser.add_argument(\"--lr_patience\", type=int, default=2, help=\"how many epoch to wait to reduce lr if mAP doesn't improve\")\n",
    "\n",
    "# parser.add_argument(\"--n-print-steps\", type=int, default=100, help=\"number of steps to print statistics\")\n",
    "# parser.add_argument('--save_model', help='save the model or not', type=ast.literal_eval)\n",
    "\n",
    "# parser.add_argument('--freqm', help='frequency mask max length', type=int, default=0)\n",
    "# parser.add_argument('--timem', help='time mask max length', type=int, default=0)\n",
    "# parser.add_argument(\"--mixup\", type=float, default=0, help=\"how many (0-1) samples need to be mixup during training\")\n",
    "# parser.add_argument(\"--bal\", type=str, default=None, help=\"use balanced sampling or not\")\n",
    "# # the stride used in patch spliting, e.g., for patch size 16*16, a stride of 16 means no overlapping, a stride of 10 means overlap of 6.\n",
    "# parser.add_argument(\"--fstride\", type=int, default=10, help=\"soft split freq stride, overlap=patch_size-stride\")\n",
    "# parser.add_argument(\"--tstride\", type=int, default=10, help=\"soft split time stride, overlap=patch_size-stride\")\n",
    "# parser.add_argument('--imagenet_pretrain', help='if use ImageNet pretrained audio spectrogram transformer model', type=ast.literal_eval, default='True')\n",
    "# parser.add_argument('--audioset_pretrain', help='if use ImageNet and audioset pretrained audio spectrogram transformer model', type=ast.literal_eval, default='False')\n",
    "\n",
    "# args = parser.parse_args(args=['--data-train','--data-val','--data-eval','--label-csv','--n_class'])\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "025bf21b-f177-418e-952f-cb5223e2b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now train a audio spectrogram transformer model\n"
     ]
    }
   ],
   "source": [
    "# transformer based model\n",
    "# if args.model == 'ast':\n",
    "print('now train a audio spectrogram transformer model')\n",
    "# dataset spectrogram mean and std, used to normalize the input\n",
    "norm_stats = {'audioset':[-4.2677393, 4.5689974], 'esc50':[-6.6268077, 5.358466], 'speechcommands':[-6.845978, 5.5654526]}\n",
    "target_length = {'audioset':1024, 'esc50':512, 'speechcommands':128}\n",
    "\n",
    "# if add noise for data augmentation, only use for speech commands\n",
    "noise = {'audioset': False, 'esc50': False, 'speechcommands':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a4c5f1-8714-466a-ae03-0ad2442e6323",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jb/6jbx60014qjb0gw4m0gctvtr0000gn/T/ipykernel_19405/3760759586.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset],\n\u001b[0m\u001b[1;32m      2\u001b[0m               \u001b[0;34m'freqm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreqm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'timem'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixup'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               \u001b[0;34m'dataset'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnorm_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnorm_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               'noise':noise[args.dataset]}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset],\n",
    "#               'freqm': args.freqm, 'timem': args.timem, 'mixup': args.mixup,\n",
    "#               'dataset': args.dataset, 'mode':'train',\n",
    "#               'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1],\n",
    "#               'noise':noise[args.dataset]}\n",
    "# val_audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset],\n",
    "#                   'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset,\n",
    "#                   'mode':'evaluation', 'mean':norm_stats[args.dataset][0], \n",
    "#                   'std':norm_stats[args.dataset][1], 'noise':False}\n",
    "\n",
    "audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset],\n",
    "              'freqm': 0, 'timem': 0, 'mixup': 0,\n",
    "              'dataset': args.dataset, 'mode':'train',\n",
    "              'mean':norm_stats[args.dataset][0], 'std':norm_stats[args.dataset][1],\n",
    "              'noise':noise[args.dataset]}\n",
    "val_audio_conf = {'num_mel_bins': 128, 'target_length': target_length[args.dataset],\n",
    "                  'freqm': 0, 'timem': 0, 'mixup': 0, 'dataset': args.dataset,\n",
    "                  'mode':'evaluation', 'mean':norm_stats[args.dataset][0], \n",
    "                  'std':norm_stats[args.dataset][1], 'noise':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99fceac9-6169-4675-857a-7043e982c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced sampler is not used\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jb/6jbx60014qjb0gw4m0gctvtr0000gn/T/ipykernel_19405/3524596444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced sampler is not used'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m train_loader = torch.utils.data.DataLoader(\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAudiosetDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# if args.bal == 'bal':\n",
    "#     print('balanced sampler is being used')\n",
    "#     samples_weight = np.loadtxt(args.data_train[:-5]+'_weight.csv', delimiter=',')\n",
    "#     sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "#     train_loader = torch.utils.data.DataLoader(\n",
    "#         dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf),\n",
    "#         batch_size=args.batch_size, sampler=sampler, num_workers=args.num_workers, pin_memory=True)\n",
    "# else:\n",
    "\n",
    "print('balanced sampler is not used')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudiosetDataset(args.data_train, label_csv=args.label_csv, audio_conf=audio_conf),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bbbcdf-e4cb-4fc0-9970-b2312b8df51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataloader.AudiosetDataset(args.data_val, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "    batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc624b-f005-480b-b476-93577f9480f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = models.ASTModel(label_dim=args.n_class, fstride=args.fstride, tstride=args.tstride, input_fdim=128,\n",
    "                              input_tdim=target_length[args.dataset], imagenet_pretrain=args.imagenet_pretrain,\n",
    "                              audioset_pretrain=args.audioset_pretrain, model_size='base384')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faad296-d280-4990-9909-af02123b60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating experiment directory: %s\" % args.exp_dir)\n",
    "os.makedirs(\"%s/models\" % args.exp_dir)\n",
    "with open(\"%s/args.pkl\" % args.exp_dir, \"wb\") as f:\n",
    "    pickle.dump(args, f)\n",
    "\n",
    "    \n",
    "print('Now starting training for {:d} epochs'.format(args.n_epochs))\n",
    "train(audio_model, train_loader, val_loader, args)# 这个train的话，当然是包括“十折交叉里面的训练数据集+验证数据集（train_loader）” + testing数据集（val_loader）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8b50c-c8ec-4030-8cf5-4a8050edc7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for speechcommands dataset, evaluate the best model on validation set on the test set\n",
    "# 后面如果涉及到评估的时候，可以用这部分的代码\n",
    "\n",
    "# if args.dataset == 'speechcommands':\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     sd = torch.load(args.exp_dir + '/models/best_audio_model.pth', map_location=device)\n",
    "#     audio_model = torch.nn.DataParallel(audio_model)\n",
    "#     audio_model.load_state_dict(sd)\n",
    "\n",
    "\n",
    "#     # best model on the validation set # 内部的验证数据集\n",
    "#     stats, _ = validate(audio_model, val_loader, args, 'valid_set')\n",
    "#     # note it is NOT mean of class-wise accuracy\n",
    "#     val_acc = stats[0]['acc']\n",
    "#     val_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "#     print('---------------evaluate on the validation set---------------')\n",
    "#     print(\"Accuracy: {:.6f}\".format(val_acc))\n",
    "#     print(\"AUC: {:.6f}\".format(val_mAUC))\n",
    "\n",
    "\n",
    "#     # test the model on the evaluation set # 独立的第三方数据集，叫做evaluation set\n",
    "#     eval_loader = torch.utils.data.DataLoader(\n",
    "#         dataloader.AudiosetDataset(args.data_eval, label_csv=args.label_csv, audio_conf=val_audio_conf),\n",
    "#         batch_size=args.batch_size*2, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "      # 从这个数据来看，input就是音频文件的路径，以及具体的分类标签\n",
    "    \n",
    "    \n",
    "#     stats, _ = validate(audio_model, eval_loader, args, 'eval_set')\n",
    "#     eval_acc = stats[0]['acc']\n",
    "#     eval_mAUC = np.mean([stat['auc'] for stat in stats])\n",
    "\n",
    "\n",
    "#     print('---------------evaluate on the test set---------------')\n",
    "#     print(\"Accuracy: {:.6f}\".format(eval_acc))\n",
    "#     print(\"AUC: {:.6f}\".format(eval_mAUC))\n",
    "#     np.savetxt(args.exp_dir + '/eval_result.csv', [val_acc, val_mAUC, eval_acc, eval_mAUC])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
